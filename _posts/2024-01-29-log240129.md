---
layout: post
title: ' NLTK 데이터 파일 저장하기 '
subtitle: ' 2024년 1월 29일 월요일 '
catalog: true
category: log
subcategory: weekly
tags:
  - log
  - weekly
  - January
  - 2024
  - python
  - s3
  - git lfs
  - nltk

---

# Today I Learned

## 날짜

2024년 1월 29일 월요일

## 내용

기능 개발이 끝나고 QA를 시작했다.

### nltk

 지난주에 nltk 패키지에서 필요한 데이터들을 templates 폴더에 추가하여 추가적인 다운로드 없이 사용하도록 코드를 작성했었다. 테스트 서버에서 오류가 발생했는데, 범인은 금요일이 연차여서 존경하는 선배님께서 원상복구 해주셨다.

 문제 해결을 위해, 고민의 원점에 서서 차근차근 생각하며 다양한 방법을 생각했다.

1. 현재 해결하고자 하는 것은 무엇인가?
    1. nltk 패키지에서 사용할 데이터 다운로드 횟수를 최소한으로 만들자.
2. 그 목적은 무엇인가?
    1. 불필요하게 반복되는 데이터 다운로드는 리소스 낭비기 때문이다.
3. 해결하기 위한 방법들은 무엇들이 있는가?
    1. templates 디렉토리 내에 데이터를 저장한다(현재).
    2. S3, Git Large File Storage 등의 스토리지 서비스를 이용한다.
    3. dockerFile에 이미지 빌드 시 필요한 데이터를 다운로드 하도록 한다.

 방법 (a)의 동작을 정상화 시킨다고 하더라도, 간과한 문제가 있었다. nltk에서 다운로드 하는 데이터는 3가지다.

- punkt : 각 문장을 서로 구별하고, 단어 별로 토큰화하기 위한 알고리즘, 데이터 등
- opinion_lexicon : 각 단어의 긍정, 부정을 판별하기 위한 단어 데이터(텍스트)
- stopwords : 분석이 필요하지 않은 불용어들의 단어 데이터(텍스트)

 위 셋의 데이터를 합치면 약 125MB가 된다. 영어 뿐 아니라 수십개의 언어 데이터가 모두 들어있고, 특히 punkt 데이터가 대부분을 차지하는데 단순 텍스트 나열이 아니라 tokenize를 위한 머신러닝 훈련 내용들이 들어있기 떄문이다(이 때문에 뒤에서 punkt는 따로 처리해야만 했다).

 templates내에 이 데이터들을 저장한다는 것은 Github repository에도 포함된다는 것인데, 형상관리 툴에서 가지고 있기엔 너무 크다(실제로 Repository에서 Git pull을 할 때 단일 파일이 50MB 이상이면 경고가 뜨고 100MB 이상이면 오류가 뜬다고 한다). 용량이 큰 파일이 git에 올라가면 git pull, clone 등에 큰 파일이 포함되어 시간도 오래걸리고, 이를 바탕으로 빌드되는 테스트서버의 배포도 느려진다. 테스트가 아닌 “불필요한” 데이터로 인해 배포 과정이 지연되는 것은 애자일 방식에서든 어떤 면에서든 좋을리가 없다. 여기서 말하는 “불필요한”은 없어도 무방하다는 뜻이 아닌, 다른 방식으로 챙길 수 있는데 굳이 포함되어 있다는 의미이다.

 Git LFS(Large File Storage)는 이번에 찾아보면서 처음 알게 되었는데, S3와 비슷한 객체 스토리지 서비스는 아니었다. 위에서 말한 것처럼, Repository에 포함되어야 할 파일이 클 경우 이를 보관히기 위한 저장 서버다. 데이터가 있어야 할 곳에는 이 저장소를 가리키는 포인터를 두어 해결한다.

 서버가 구동되며 가져오거나 참조할 데이터의 저장소가 필요했으므로 기존에 사용하던 S3 버킷이 가장 적절하다고 생각했다. 서버와 S3가 같은 리전에 있으면 데이터를 주고받는데 비용이 들지 않지만, 노파심에 데이터를 줄이고자 영어를 제외한 다른 언어 파일은 모두 제거했다. aliexpress에서 리뷰를 import할 때 영어로 번역되어 넘어오기 떄문이다.

 punkt는 상술하였듯, 단순 텍스트파일이 아니기 떄문에 s3에서 가져올 수 없었다. 따라서 Dockerfile에서 빌드할 때 이미지에 다운로드하도록 추가했다.

### AI

 상품 리뷰들을 요약하기 위해 AI에게 몇몇 데이터와 명령문을 제공했다. 입력하는 양, 받는 결과 한글자마다 다 돈이기 때문에 최대한 간단하게 입력하여 정확한 결과를 산출하는게 중요했다. 결국 만드는 것은 “요약”이기 때문에, 너무 길면 안된다고 생각하여 분량에 제한을 뒀다. 

 AI에게 제공하는 데이터 중 하나는 상품명인데, Aliexpress에 들어가보면 알겠지만 상품명이 너무 길다. 키보드라면 “ 길이가 30cm 이고 누르면 컴퓨터에 글자가 입력되며 상황에 따라 불빛을 바꿀 수 있는 키보드” 가 상품명인 느낌이랄까… 요약문에 “이 상품은 …”으로 시작되는 것과 “길이가 30cm 이고 …” 로 시작되는 것은 비용측면에서도 좋지 않을 뿐더러 불필요한 글자수 낭비로 한줄평이 완성되지 못하고 짤리는 결과를 도출했다.

 따라서, AI에게 요약문에서 상품 이름은 “This product”라고 지칭하도록 더 명확히 말해주었다.

## 회고

 nltk 데이터를 어디다 둬야할 지 고민할 떄 파일의 용량으로 인한 효과는 고려하지 못했었다. 아는 만큼 보이는 법.
